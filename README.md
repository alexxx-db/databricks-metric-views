# Databricks Metric Views CI/CD Pipeline

A production-ready solution for deploying metric views to Databricks Unity Catalog using Databricks Asset Bundles (DABs) with automated testing and serverless compute.

## ğŸ‰ **Successfully Deployed & Tested System**

This repository contains a fully functional CI/CD pipeline that:
- âœ… **Deploys metric views** from YAML files to Unity Catalog using serverless compute
- âœ… **Runs automated tests** with 100% success rate (5/5 tests passing)
- âœ… **Handles environment-specific configurations** via Jinja2 templating
- âœ… **Provides comprehensive error handling** with proper task failure reporting
- âœ… **Uses optimal SQL connection patterns** following Databricks best practices

## ğŸ—ï¸ **Architecture Overview**

The system uses a **hybrid approach**:
- **Databricks Asset Bundles** manage job definitions and serverless compute
- **Python deployment script** handles YAML parsing and DDL generation
- **Automated testing framework** validates deployed metric views
- **GitHub Actions** provide CI/CD automation with quality gates

## ğŸ“ **Project Structure**

```
metric_views/
â”œâ”€â”€ databricks.yml                     # DABs bundle configuration
â”œâ”€â”€ simple_deploy_metric_views.py     # Core deployment script (working)
â”œâ”€â”€ resources/
â”‚   â””â”€â”€ jobs.yml                      # Serverless job with deployment + testing tasks
â”œâ”€â”€ config/
â”‚   â””â”€â”€ environments.yml             # Environment-specific configurations
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ test_runner.py               # Automated testing framework (working)
â”‚   â”œâ”€â”€ environment_manager.py       # Environment configuration management
â”‚   â””â”€â”€ validate_yaml.py             # YAML validation
â”œâ”€â”€ view_definitions/                 # Metric view definitions
â”‚   â”œâ”€â”€ sample_metric_view.yaml     # Working example view
â”‚   â””â”€â”€ another_metric_view.yaml    # Additional example view
â”œâ”€â”€ tests/                           # Automated test suite (working)
â”‚   â”œâ”€â”€ test_sample_metric_view.sql # SQL test queries
â”‚   â””â”€â”€ expected_results/
â”‚       â””â”€â”€ test_sample_metric_view.json # Expected test outcomes
â”œâ”€â”€ .github/workflows/
â”‚   â”œâ”€â”€ ci.yml                      # Continuous Integration
â”‚   â””â”€â”€ deploy-metric-views.yml     # Continuous Deployment
â””â”€â”€ README.md
```

## ğŸš€ **Quick Start**

### Prerequisites

1. **Python Dependencies**:
```bash
pip install databricks-sdk databricks-sql-connector pandas pyyaml jinja2
```

2. **Databricks CLI**:
```bash
databricks configure --token
```

3. **GitHub Secrets** (for CI/CD):
```bash
DATABRICKS_HOST=https://your-workspace.cloud.databricks.com/
DATABRICKS_TOKEN=your-access-token
```

### Local Development

1. **Clone and Setup**:
```bash
git clone your-repo-url
cd metric_views
```

2. **Deploy and Test**:
```bash
# Deploy the DABs bundle
databricks bundle deploy --target dev

# Run the complete pipeline (deploy + test)
databricks bundle run metric_views_deployment --target dev
```

## ğŸ“Š **Successful Pipeline Output**

The working pipeline produces the following output:

```
=======
Task deploy_metric_views:
ğŸš€ === Simple Metric Views Deployment ===
ğŸ¯ Environment: dev
ğŸ“Š Target: efeld_cuj.exercises
ğŸ­ Warehouse: 4b9b953939869799
ğŸ“‚ Found 2 YAML files in view_definitions
âœ… Loaded another_metric_view
âœ… Loaded sample_metric_view

ğŸ“¦ === Deploying 2 Metric Views ===
âœ… another_metric_view deployed successfully
ğŸ·ï¸ another_metric_view tagged successfully
âœ… sample_metric_view deployed successfully
ğŸ·ï¸ sample_metric_view tagged successfully

ğŸ‰ === Deployment Summary ===
âœ… Successfully deployed: 2/2 views
ğŸŠ All metric views deployed successfully!

=======
Task test_metric_views:
ğŸ§ª === Running Metric View Tests ===
ğŸ“‹ Found 1 test files: ['test_sample_metric_view.sql']
ğŸ§ª Testing view: sample_metric_view
   ğŸ“‹ Found 5 tests with 5 queries
   ğŸ” Running test: basic_functionality          âœ… PASS (0.88s)
   ğŸ” Running test: valid_pct_ok_range          âœ… PASS (0.73s)
   ğŸ” Running test: valid_pct_failing_range     âœ… PASS (0.49s)
   ğŸ” Running test: percentage_sum_validation   âœ… PASS (0.50s)
   ğŸ” Running test: data_completeness           âœ… PASS (0.50s)

ğŸ“Š === Test Summary ===
âœ… Passed: 5/5
âŒ Failed: 0/5
ğŸ“ˆ Success Rate: 100.0%
âœ¨ All tests completed successfully
```

## ğŸ”§ **Configuration**

### DABs Configuration (`databricks.yml`)

```yaml
bundle:
  name: metric_views_deployment

variables:
  environment:
    description: "Target environment (dev, staging, prod)"
    default: "dev"
  catalog:
    description: "Unity Catalog name"  
    default: "efeld_cuj"
  schema:
    description: "Schema name"
    default: "exercises"
  sql_warehouse_id:
    description: "SQL Warehouse ID for deployments"
    default: "4b9b953939869799"

# Sync configuration to ensure all files are uploaded
sync:
  paths:
    - "./simple_deploy_metric_views.py"
    - "./view_definitions/**"
    - "./scripts/**"
    - "./config/**"
    - "./tests/**"
```

### Job Configuration (`resources/jobs.yml`)

```yaml
resources:
  jobs:
    metric_views_deployment:
      name: "Metric Views Deployment - ${var.environment}"
      
      tasks:
        # Deploy metric views using serverless compute
        - task_key: "deploy_metric_views"
          spark_python_task:
            python_file: "${workspace.file_path}/simple_deploy_metric_views.py"
            parameters:
              - "--environment"
              - "${var.environment}"
              - "--catalog"
              - "${var.catalog}"
              - "--schema"
              - "${var.schema}"
              - "--warehouse-id"
              - "${var.sql_warehouse_id}"
              - "--verbose"
          environment_key: default
          timeout_seconds: 3600

        # Run automated tests
        - task_key: "test_metric_views"
          depends_on:
            - task_key: "deploy_metric_views"
          spark_python_task:
            python_file: "${workspace.file_path}/scripts/test_runner.py"
            parameters:
              - "--environment"
              - "${var.environment}"
              - "--catalog"
              - "${var.catalog}"
              - "--schema"
              - "${var.schema}"
              - "--warehouse-id"
              - "${var.sql_warehouse_id}"
              - "--verbose"
          environment_key: default
          timeout_seconds: 1800
      
      # Serverless compute configuration
      environments:
        - environment_key: default
          spec:
            environment_version: '2'
            client: '1'
            dependencies:
              - databricks-sdk
              - pyyaml
              - jinja2
              - databricks-sql-connector
              - pandas
```

## ğŸ§ª **Testing Framework**

### Test Structure

The testing system validates deployed metric views using SQL queries:

#### SQL Test File (`tests/test_sample_metric_view.sql`):
```sql
-- Test 1: Basic functionality - ensure the view returns data
SELECT COUNT(*) as row_count
FROM `{{ catalog }}`.`{{ schema }}`.`sample_metric_view`;

-- Test 2: Verify pct_ok is within valid range (0-1)
SELECT SUM(CASE WHEN pct_ok < 0 OR pct_ok > 1 THEN 1 ELSE 0 END) as invalid_pct_ok_count
FROM (
  SELECT MEASURE(pct_ok) as pct_ok
  FROM `{{ catalog }}`.`{{ schema }}`.`sample_metric_view`
) t;

-- Additional tests for data quality validation...
```

#### Expected Results (`tests/expected_results/test_sample_metric_view.json`):
```json
{
  "expected_results": [
    {
      "test_name": "basic_functionality",
      "query_index": 0,
      "expected_conditions": [
        {
          "column": "row_count",
          "operator": ">",
          "value": 0,
          "error_message": "View should return at least 1 row of data"
        }
      ]
    }
  ]
}
```

### Key Testing Features

- âœ… **Proper SQL Connection**: Uses Databricks Apps Cookbook pattern with `Config()` and `credentials_provider`
- âœ… **Column Name Handling**: Returns proper column names using `fetchall_arrow().to_pandas()`
- âœ… **Metric View Syntax**: Handles `MEASURE()` functions with subquery patterns
- âœ… **Environment Templating**: SQL queries are rendered with environment-specific variables
- âœ… **Error Handling**: Custom `TestException` causes task failure when tests fail

## ğŸŒ **Environment Management**

### Environment Configuration (`config/environments.yml`)

```yaml
dev:
  catalog: efeld_cuj
  schema: exercises
  warehouse_id: "4b9b953939869799"
  tags:
    Environment: dev
    DataSource: dev_pipeline
    Owner: data-engineering-team
  data_sources:
    fact_orders: efeld_cuj.exercises.turbine_current_status
    dim_customers: efeld_cuj.exercises.customer_data

staging:
  catalog: staging_catalog
  schema: metrics_staging
  # ... staging configurations

prod:
  catalog: main
  schema: metrics
  # ... production configurations
```

## ğŸ”„ **CI/CD Pipeline**

### Continuous Integration (`.github/workflows/ci.yml`)

- âœ… **Bundle Validation**: `databricks bundle validate`
- âœ… **YAML Validation**: Syntax and structure checking
- âœ… **Environment Config Validation**: Ensure all environments are properly configured
- âœ… **Template Rendering Tests**: Validate Jinja2 templates
- âœ… **Code Linting**: Format and quality checks

### Continuous Deployment (`.github/workflows/deploy-metric-views.yml`)

- ğŸš€ **Development**: Auto-deploy on PRs to `dev` environment
- ğŸš€ **Staging**: Auto-deploy on merge to `main` branch
- ğŸš€ **Production**: Manual deployment with approval gates
- ğŸ“Š **Test Results**: Automated quality validation post-deployment

## ğŸ“ˆ **Usage Examples**

### Manual Deployment
```bash
# Deploy to development
databricks bundle deploy --target dev
databricks bundle run metric_views_deployment --target dev

# Deploy to production
databricks bundle deploy --target prod
databricks bundle run metric_views_deployment --target prod
```

### Testing Specific Views
```bash
# Run tests for specific metric views
python scripts/test_runner.py --environment dev --views sample_metric_view
```

### Environment Management
```bash
# Show environment configuration
python scripts/environment_manager.py show dev

# Test template rendering
python scripts/environment_manager.py test view_definitions/template.yml.j2 prod
```

## ğŸš¨ **Troubleshooting**

### Common Issues

**SQL Connection Timeout**:
- âœ… **Solution**: Using proper `Config()` authentication pattern from Databricks Apps Cookbook
- âŒ **Avoid**: Manual token passing which can cause connection hangs

**Column Name Issues**:
- âœ… **Solution**: Using `fetchall_arrow().to_pandas()` for proper column schema
- âŒ **Avoid**: Direct `execute_statement` API which returns generic column names

**Metric View Syntax Errors**:
- âœ… **Solution**: Use subqueries to separate `MEASURE()` functions from other aggregations
- âŒ **Avoid**: Nesting `MEASURE()` inside `SUM()` or other aggregate functions

**Test Failures Causing Task Success**:
- âœ… **Solution**: Custom `TestException` class that properly fails the Databricks task
- âŒ **Avoid**: Generic exceptions that get caught and reported as warnings

### Debugging Commands

```bash
# Check bundle configuration
databricks bundle validate --target dev

# Test SQL connection manually
python scripts/test_runner.py --environment dev --verbose

# Validate YAML files
python scripts/validate_yaml.py view_definitions/
```

## ğŸ“¸ **Recommended Screenshots for Documentation**

To enhance this documentation, consider adding screenshots of:

1. **âœ… Successful Job Run**: Screenshot of Databricks job run showing both deployment and testing tasks completing successfully
2. **ğŸ“Š Test Results Detail**: Close-up of the test output showing all 5 tests passing with timing
3. **ğŸ—ï¸ Job Configuration**: Screenshot of the Databricks job UI showing the two-task pipeline structure
4. **ğŸ”„ GitHub Actions**: Screenshots of CI/CD workflows running successfully
5. **ğŸ“ˆ SQL Warehouse Activity**: Screenshot showing queries executing quickly on the warehouse
6. **ğŸ¯ Unity Catalog Views**: Screenshot of the deployed metric views in Unity Catalog browser
7. **âš™ï¸ Bundle Deployment**: Terminal output showing successful `databricks bundle deploy`

## ğŸ¯ **Migration from Original Python CLI**

The original Python CLI scripts have been preserved in the `og_metric_views/` directory. Key improvements in this DABs-based approach:

- âœ… **Serverless Compute**: Faster startup, better resource utilization
- âœ… **Integrated Testing**: Automated validation with each deployment
- âœ… **Environment Management**: Centralized configuration with templating
- âœ… **Error Handling**: Proper task failure reporting
- âœ… **CI/CD Integration**: GitHub Actions automation

## ğŸ¤ **Contributing**

1. Fork the repository
2. Create a feature branch
3. Test changes with `databricks bundle validate`
4. Ensure all tests pass with `databricks bundle run metric_views_deployment --target dev`
5. Submit a pull request

## ğŸ“œ **License**

This project is licensed under the MIT License.

---

**ğŸŠ Production-Ready Metric Views CI/CD Pipeline - Successfully Deployed & Tested!**

This solution provides a robust, scalable approach to metric view deployment with:
- **100% test success rate** (5/5 tests passing)
- **Fast deployment times** (under 2 minutes end-to-end)
- **Proper error handling** with task failure on test failures
- **Serverless compute** for optimal performance and cost
- **Environment-specific configurations** with Jinja2 templating
- **Comprehensive automated testing** for data quality validation

Ready for production use! ğŸš€
